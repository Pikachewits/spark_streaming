[student685_1@bigdataanalytics-worker-3 ~]$ pyspark
SPARK_MAJOR_VERSION is set to 2, using Spark2
Python 2.7.5 (default, Nov 16 2020, 22:23:17)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/01/17 14:43:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.2.3.1.4.0-315
      /_/

Using Python version 2.7.5 (default, Nov 16 2020 22:23:17)
SparkSession available as 'spark'.

>>> from pyspark.sql import functions as F
>>> raw_rate = spark \
...     .readStream \
...     .format("rate") \
...     .load()
>>> raw_rate.printSchema()
root
 |-- timestamp: timestamp (nullable = true)
 |-- value: long (nullable = true)

>>> raw_rate.isStreaming
True
>>> stream = raw_rate \
...     .writeStream \
...     .trigger(processingTime='30 seconds') \
...     .format("console") \
...     .options(truncate=False) \
...     .start()
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+---------+-----+
|timestamp|value|
+---------+-----+
+---------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2022-01-17 14:46:46.517|0    |
|2022-01-17 14:46:47.517|1    |
|2022-01-17 14:46:48.517|2    |
|2022-01-17 14:46:49.517|3    |
|2022-01-17 14:46:50.517|4    |
|2022-01-17 14:46:51.517|5    |
|2022-01-17 14:46:52.517|6    |
|2022-01-17 14:46:53.517|7    |
|2022-01-17 14:46:54.517|8    |
|2022-01-17 14:46:55.517|9    |
|2022-01-17 14:46:56.517|10   |
|2022-01-17 14:46:57.517|11   |
|2022-01-17 14:46:58.517|12   |
+-----------------------+-----+

stream.stop()-------------------------------------------
Batch: 2
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2022-01-17 14:46:59.517|13   |
|2022-01-17 14:47:00.517|14   |
|2022-01-17 14:47:01.517|15   |
|2022-01-17 14:47:02.517|16   |
|2022-01-17 14:47:03.517|17   |
|2022-01-17 14:47:04.517|18   |
|2022-01-17 14:47:05.517|19   |
|2022-01-17 14:47:06.517|20   |
|2022-01-17 14:47:07.517|21   |
|2022-01-17 14:47:08.517|22   |
|2022-01-17 14:47:09.517|23   |
|2022-01-17 14:47:10.517|24   |
|2022-01-17 14:47:11.517|25   |
|2022-01-17 14:47:12.517|26   |
|2022-01-17 14:47:13.517|27   |
|2022-01-17 14:47:14.517|28   |
|2022-01-17 14:47:15.517|29   |
|2022-01-17 14:47:16.517|30   |
|2022-01-17 14:47:17.517|31   |
|2022-01-17 14:47:18.517|32   |
+-----------------------+-----+
only showing top 20 rows

    stream.stop()
>>> stream.explain()
== Physical Plan ==
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@d95920b
+- Scan ExistingRDD[timestamp#42,value#43L]

>>> stream.lastProgress

{u'stateOperators': [], u'name': None, u'timestamp': u'2022-01-17T14:47:30.001Z', u'processedRowsPerSecond': 188.67924528301887, u'inputRowsPerSecond': 0.9999666677777407, u'numInputRows': 30, u'batchId': 2, u'sources': [{u'description': u'RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=2]', u'endOffset': 43, u'processedRowsPerSecond': 188.67924528301887, u'inputRowsPerSecond': 0.9999666677777407, u'numInputRows': 30, u'startOffset': 13}], u'durationMs': {u'queryPlanning': 7, u'getOffset': 0, u'addBatch': 110, u'getBatch': 10, u'walCommit': 31, u'triggerExecution': 159}, u'runId': u'76947558-8fe6-4025-bab4-bf791c099d29', u'id': u'431e09d6-0b46-499f-93c6-c7f37095e17f', u'sink': {u'description': u'org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@4d09f98e'}}
>>> stream.status

{u'message': u'Stopped', u'isTriggerActive': False, u'isDataAvailable': False}
>>> def console_output(df, freq):
...     return df.writeStream \
...         .format("console") \
...         .trigger(processingTime='%s seconds' % freq ) \
...         .options(truncate=False) \
...         .start()
...
>>> filtered_rate = raw_rate \
...     .filter( F.col("value") % F.lit("2") == 0 )
>>> out = console_output(filtered_rate, 5)
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+---------+-----+
|timestamp|value|
+---------+-----+
+---------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2022-01-17 15:22:14.419|0    |
|2022-01-17 15:22:16.419|2    |
|2022-01-17 15:22:18.419|4    |
+-----------------------+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2022-01-17 15:22:20.419|6    |
|2022-01-17 15:22:22.419|8    |
+-----------------------+-----+

-------------------------------------------
Batch: 3
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2022-01-17 15:22:24.419|10   |
|2022-01-17 15:22:26.419|12   |
|2022-01-17 15:22:28.419|14   |
+-----------------------+-----+

-------------------------------------------
Batch: 4
-------------------------------------------
+-----------------------+-----+
|timestamp              |value|
+-----------------------+-----+
|2022-01-17 15:22:30.419|16   |
|2022-01-17 15:22:32.419|18   |
+-----------------------+-----+

out.stop()
>>> extra_rate = filtered_rate \
...     .withColumn("my_value",
...                 F.when((F.col("value") % F.lit(10) == 0), F.lit("jubilee"))
...                     .otherwise(F.lit("not yet")))
>>> out = console_output(extra_rate, 5)
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+---------+-----+--------+
|timestamp|value|my_value|
+---------+-----+--------+
+---------+-----+--------+

-------------------------------------------
Batch: 1
-------------------------------------------
+---------------------+-----+--------+
|timestamp            |value|my_value|
+---------------------+-----+--------+
|2022-01-17 15:23:48.7|0    |jubilee |
+---------------------+-----+--------+

-------------------------------------------
Batch: 2
-------------------------------------------
+---------------------+-----+--------+
|timestamp            |value|my_value|
+---------------------+-----+--------+
|2022-01-17 15:23:50.7|2    |not yet |
|2022-01-17 15:23:52.7|4    |not yet |
+---------------------+-----+--------+

-------------------------------------------
Batch: 3
-------------------------------------------
+---------------------+-----+--------+
|timestamp            |value|my_value|
+---------------------+-----+--------+
|2022-01-17 15:23:54.7|6    |not yet |
|2022-01-17 15:23:56.7|8    |not yet |
|2022-01-17 15:23:58.7|10   |jubilee |
+---------------------+-----+--------+

-------------------------------------------
Batch: 4
-------------------------------------------
+---------------------+-----+--------+
|timestamp            |value|my_value|
+---------------------+-----+--------+
|2022-01-17 15:24:00.7|12   |not yet |
|2022-01-17 15:24:02.7|14   |not yet |
+---------------------+-----+--------+

out.stop()
>>> def killAll():
...     for active_stream in spark.streams.active:
...         print("Stopping %s by killAll" % active_stream)
...         active_stream.stop()
...
>>> console_output(extra_rate, 5)
<pyspark.sql.streaming.StreamingQuery object at 0x7f11e76d0d50>
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+---------+-----+--------+
|timestamp|value|my_value|
+---------+-----+--------+
+---------+-----+--------+

-------------------------------------------
Batch: 1
-------------------------------------------
+-----------------------+-----+--------+
|timestamp              |value|my_value|
+-----------------------+-----+--------+
|2022-01-17 15:31:46.592|0    |jubilee |
|2022-01-17 15:31:48.592|2    |not yet |
+-----------------------+-----+--------+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----------------------+-----+--------+
|timestamp              |value|my_value|
+-----------------------+-----+--------+
|2022-01-17 15:31:50.592|4    |not yet |
|2022-01-17 15:31:52.592|6    |not yet |
+-----------------------+-----+--------+

-------------------------------------------
Batch: 3
-------------------------------------------
+-----------------------+-----+--------+
|timestamp              |value|my_value|
+-----------------------+-----+--------+
|2022-01-17 15:31:54.592|8    |not yet |
|2022-01-17 15:31:56.592|10   |jubilee |
|2022-01-17 15:31:58.592|12   |not yet |
+-----------------------+-----+--------+

-------------------------------------------
Batch: 4
-------------------------------------------
+-----------------------+-----+--------+
|timestamp              |value|my_value|
+-----------------------+-----+--------+
|2022-01-17 15:32:00.592|14   |not yet |
|2022-01-17 15:32:02.592|16   |not yet |
+-----------------------+-----+--------+

killAll()
Stopping <pyspark.sql.streaming.StreamingQuery object at 0x7f11e76c2f50> by killAll
>>> exit()
[student685_1@bigdataanalytics-worker-3 ~]$ exit
logout

──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────


