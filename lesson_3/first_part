[student685_1@bigdataanalytics-worker-3 for_stream]$ ls
iris.json  pokemon_1.csv  pokemon_2.csv  pokemon_3.csv
[student685_1@bigdataanalytics-worker-3 for_stream]$ pyspark
SPARK_MAJOR_VERSION is set to 2, using Spark2
Python 2.7.5 (default, Nov 16 2020, 22:23:17)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/01/24 16:48:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
22/01/24 16:48:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.2.3.1.4.0-315
      /_/

Using Python version 2.7.5 (default, Nov 16 2020 22:23:17)
SparkSession available as 'spark'.
>>> from pyspark.sql import functions as F
>>> from pyspark.sql.types import StructType, StringType
>>> def console_output(df, freq):
...     return df.writeStream \
...             .format("console") \
...             .trigger(processingTime='%s seconds' % freq) \
...             .options(truncate=False) \
...             .start()
...
>>> schema = StructType() \
...     .add("Name", StringType()) \
...     .add("Total", StringType()) \
...     .add("HP", StringType()) \
...
>>> raw_files = spark \
...     .readStream \
...     .format("csv") \
...     .schema(schema) \
...     .options(path="input_csv_for_stream", header=True) \
...     .load()
>>> out = console_output(raw_files, 5)
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------+-----+----+
|Name          |Total|HP  |
+--------------+-----+----+
|Bulbasaur     |318  |45  |
|Ivysaur       |405  |60  |
|Venusaur      |525  |80  |
|Mega Venusaur |625  |80  |
|Charmander    |309  |39  |
|Charmeleon    |405  |58  |
|Charizard     |534  |78  |
|Mega Charizard|634  |null|
+--------------+-----+----+

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------+-----+---+
|Name          |Total|HP |
+--------------+-----+---+
|Squirtle      |314  |44 |
|Wartortle     |405  |59 |
|Blastoise     |530  |79 |
|Mega Blastoise|630  |79 |
|Caterpie      |195  |45 |
|Metapod       |205  |50 |
|Butterfree    |395  |60 |
|Weedle        |195  |40 |
|Kakuna        |205  |45 |
|Pidgey        |251  |40 |
|Pidgeotto     |349  |63 |
|Pidgeot       |479  |83 |
|Mega Pidgeot  |579  |83 |
|Rattata       |253  |30 |
|Mega Rattata  |253  |30 |
|Raticate      |413  |55 |
|Mega Raticate |413  |75 |
+--------------+-----+---+

out.stop()
>>> raw_files = spark \
...     .readStream \
...     .format("csv") \
...     .schema(schema) \
...     .options(path="input_csv_for_stream", header=True, maxFilesPerTrigger=1)                                                                               \
...     .load()
>>> extra_files = raw_files \
...     .withColumn("MyColumn", F.col("HP"))
>>> out = console_output(extra_files, 5)
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+--------------+-----+----+--------+
|Name          |Total|HP  |MyColumn|
+--------------+-----+----+--------+
|Bulbasaur     |318  |45  |45      |
|Ivysaur       |405  |60  |60      |
|Venusaur      |525  |80  |80      |
|Mega Venusaur |625  |80  |80      |
|Charmander    |309  |39  |39      |
|Charmeleon    |405  |58  |58      |
|Charizard     |534  |78  |78      |
|Mega Charizard|634  |null|null    |
+--------------+-----+----+--------+

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------+-----+---+--------+
|Name          |Total|HP |MyColumn|
+--------------+-----+---+--------+
|Squirtle      |314  |44 |44      |
|Wartortle     |405  |59 |59      |
|Blastoise     |530  |79 |79      |
|Mega Blastoise|630  |79 |79      |
|Caterpie      |195  |45 |45      |
|Metapod       |205  |50 |50      |
|Butterfree    |395  |60 |60      |
|Weedle        |195  |40 |40      |
|Kakuna        |205  |45 |45      |
+--------------+-----+---+--------+

-------------------------------------------
Batch: 2
-------------------------------------------
+-------------+-----+---+--------+
|Name         |Total|HP |MyColumn|
+-------------+-----+---+--------+
|Pidgey       |251  |40 |40      |
|Pidgeotto    |349  |63 |63      |
|Pidgeot      |479  |83 |83      |
|Mega Pidgeot |579  |83 |83      |
|Rattata      |253  |30 |30      |
|Mega Rattata |253  |30 |30      |
|Raticate     |413  |55 |55      |
|Mega Raticate|413  |75 |75      |
+-------------+-----+---+--------+

out.stop()
>>> exit()
[student685_1@bigdataanalytics-worker-3 for_stream]$ hdfs dfs -rm -r input_csv_for_stream
22/01/24 17:05:56 INFO fs.TrashPolicyDefault: Moved: 'hdfs://bigdataanalytics-head-0.mcs.local:8020/user/student685_1/input_csv_for_stream' to trash at: hdfs://                                                                              bigdataanalytics-head-0.mcs.local:8020/user/student685_1/.Trash/Current/user/stu                                                                              dent685_1/input_csv_for_stream
[student685_1@bigdataanalytics-worker-3 for_stream]$

